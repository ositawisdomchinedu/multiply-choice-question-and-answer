{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c2c2d9f",
   "metadata": {},
   "source": [
    "# MCQ Generator \n",
    "\n",
    "This notebook generates **high-quality multiple-choice questions (MCQs)** from PDF documents via **LangChain** using:\n",
    "- **openai/gpt-oss-120b** for MCQ generation + scoring\n",
    "- **Sentence Transformers** for fast semantic filtering\n",
    "- **Async + batching** for speed\n",
    "- **Early stopping** so generation halts once enough MCQs are collected\n",
    "\n",
    "\n",
    "## Features\n",
    "- Input: PDF document\n",
    "- Scalable chunking of text\n",
    "- MCQ generation (1 correct + 3–4 distractors)\n",
    "- SentenceTransformer-based semantic filtering (Save API cost)\n",
    "- Early stopping (don’t process all chunks unnecessarily)\n",
    "- LLM-based scoring (batched for speed)\n",
    "- Difficulty management (`easy`, `medium`, `hard`)\n",
    "- Output: JSON file with all generated MCQs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843b5fa",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab444e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\game4\\Desktop\\assignment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7fabce",
   "metadata": {},
   "source": [
    "### Logging and API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b5a00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 17:25:21,923 | INFO | GROQ_API_KEY loaded from .env\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "# Load env vars from .env in project root\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure GOOGLE_API_KEY is visible to langchain-google-genai\n",
    "#os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    logging.warning(\"GROQ_API_KEY is not set. Create a .env file with GOOGLE_API_KEY=your_key\")\n",
    "else:\n",
    "    logging.info(\"GROQ_API_KEY loaded from .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a092f3f",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe86428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = {\n",
    "    \"pdf_path\": \"notes.pdf\",        # your PDF data\n",
    "    \"max_questions\": 10,            # final cap after filtering and removing duplicate questions\n",
    "    \"chunk_size\": 500,             \n",
    "    \"chunk_overlap\": 50,         \n",
    "    \"model\": \"openai/gpt-oss-120b\",\n",
    "    \"temperature\": 0.6,\n",
    "    \"batch_size\": 5,                # MCQs per scoring request (batched)\n",
    "    \"max_workers\": 5,               # parallel Gemini calls for generation\n",
    "    \"skip_pages\": 5,                # Number of pages to skip from the start (e.g., TOC)\n",
    "\n",
    "    # Difficulty MCQ set (Bloom’s Taxonomy)\n",
    "    \"difficulty_distribution\": \n",
    "    {\n",
    "         \"easy\": 2,     \n",
    "        \"medium\": 4,  \n",
    "        \"hard\": 4    \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1cf38a",
   "metadata": {},
   "source": [
    "###  Load and Chunk PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f960e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_chunk_pdf(path, chunk_size, overlap, skip_pages=0):\n",
    "    \"\"\"\n",
    "    Load a PDF and split it into overlapping text chunks, skipp unwanted pages(Table of Content) \n",
    "    only want MCQs to come from Chapter One onward.\n",
    "    \n",
    "    Args:\n",
    "        path(str): Path to PDF file.\n",
    "        chunk_size(int): Maximum characters per chunk.\n",
    "        overlap(int): Number of characters to overlap between chunks.\n",
    "        skip_pages (int): Number of initial pages to skip.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        List of chunked documents.\n",
    "    \"\"\"\n",
    "\n",
    "    loader = PyPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Keep only pages starting from 6 (0-based index, so skip pages 0–4)\n",
    "    # Only want pages starting from Chapter One onward.\n",
    "    \n",
    "    docs = [doc for doc in docs if doc.metadata[\"page\"] >= skip_pages]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    logging.info(f\"Loaded {len(chunks)} chunks from {path} (skipping pages {skip_pages} pages)\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e444d",
   "metadata": {},
   "source": [
    "### llm setup (Prompt Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b2b8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\game4\\AppData\\Local\\Temp\\ipykernel_30636\\220721891.py:41: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  mcq_chain = LLMChain(llm=llm, prompt=mcq_prompt)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Difficulty hints guide LLM to produce appropriate question types\n",
    "DIFFICULTY_HINT = {\n",
    "    \"easy\": \"Recall and understanding; avoid tricky distractors.\",\n",
    "    \"medium\": \"Application and analysis; include plausible but incorrect distractors.\",\n",
    "    \"hard\": \"Evaluation and synthesis; nuanced distractors testing deeper understanding.\"\n",
    "}\n",
    "\n",
    "# Initialize Gemini LLM via LangChain wrapper\n",
    "#llm = ChatGoogleGenerativeAI(model=cfg[\"model\"], temperature=cfg[\"temperature\"])\n",
    "llm = ChatGroq(model=cfg[\"model\"], temperature=cfg[\"temperature\"])\n",
    "\n",
    "# Prompt: instruct the model to return strict JSON only\n",
    "mcq_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"difficulty_hint\"],\n",
    "    template=\"\"\"\n",
    "You are an expert educator. Generate up to 3 multiple-choice questions from this text:\n",
    "\n",
    "{context}\n",
    "\n",
    "Difficulty instruction: {difficulty_hint}\n",
    "\n",
    "Return STRICT JSON:\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"string\",\n",
    "    \"choices\": [\"string\", \"string\", \"string\", \"string\"],\n",
    "    \"answer\": \"string\",\n",
    "    \"explanation\": \"string\"\n",
    "  }}\n",
    "]\n",
    "Rules:\n",
    "- One correct answer from the text\n",
    "- 3–4 plausible distractors (no 'All/None of the above')\n",
    "- The choices should start with A., B., C., and D.\n",
    "- Answer must appear in choices\n",
    "- Avoid True/False\n",
    "- Return JSON only (no extra text)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "mcq_chain = LLMChain(llm=llm, prompt=mcq_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021e492",
   "metadata": {},
   "source": [
    "### Validation & Deduplication Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb33060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_item(it):\n",
    "    \"\"\"\n",
    "    Basic validation of an MCQ dict.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the item has required fields and is sane.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all(k in it for k in [\"question\", \"choices\", \"answer\", \"explanation\"]):\n",
    "        return False\n",
    "    if not isinstance(it[\"choices\"], list) or len(it[\"choices\"]) < 3:\n",
    "        return False\n",
    "    if it[\"answer\"] not in it[\"choices\"]:\n",
    "        return False\n",
    "    # basic text sanity\n",
    "    if not it[\"question\"].strip():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def dedupe_items(items):\n",
    "    \"\"\"\n",
    "    Remove duplicate questions by normalized question text.\n",
    "\n",
    "    Args:\n",
    "        items (list): list of MCQ dicts\n",
    "\n",
    "    Returns:\n",
    "        list: deduplicated list preserving first occurrences\n",
    "    \"\"\"\n",
    "    seen, out = set(), []\n",
    "    for it in items:\n",
    "        q = \" \".join(it[\"question\"].split()).strip().lower()\n",
    "        if q not in seen:\n",
    "            seen.add(q)\n",
    "            out.append(it)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085013bb",
   "metadata": {},
   "source": [
    "### Semantic Filter (SentenceTransformer Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19523077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 17:25:22,206 | INFO | Use pytorch device_name: cpu\n",
      "2025-09-07 17:25:22,210 | INFO | Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize sentence Transformer\n",
    "sent_embed = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def semantic_filter(items, context, threshold=0.25):\n",
    "    \"\"\"\n",
    "    Keep MCQs where at least one choice is semantically related to the context.\n",
    "    Uses sentence-transformer embeddings.\n",
    "\n",
    "     Args:\n",
    "        items (list): MCQ dicts\n",
    "        context (str): chunk text\n",
    "        threshold (float): cosine similarity threshold (0-1)\n",
    "\n",
    "    Returns:\n",
    "        list: filtered MCQs\n",
    "        \n",
    "    \"\"\"\n",
    "    if not items:\n",
    "        return items\n",
    "\n",
    "    context_vec = sent_embed.encode([context])[0]\n",
    "    out = []\n",
    "    for it in items:\n",
    "        # evaluate all choices vs context; allow one sufficiently-related choice\n",
    "        choice_vecs = sent_embed.encode(it[\"choices\"])\n",
    "        sims = cosine_similarity([context_vec], choice_vecs).flatten()\n",
    "        if max(sims) >= threshold:\n",
    "            out.append(it)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47817ab",
   "metadata": {},
   "source": [
    "### Parallel MCQ Generation (with Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dcb45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_mcqs_async(chunks, difficulty, max_questions):\n",
    "    \"\"\"\n",
    "    Generate MCQs in parallel across chunks using a thread pool.\n",
    "    Stops early once max_questions is reached.\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    collected = []\n",
    "\n",
    "    def process_chunk(chunk):\n",
    "        \"\"\"\n",
    "        Blocking processing for a single chunk \n",
    "        Returns a list of filtered MCQs for that chunk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            res = mcq_chain.run({\n",
    "                \"context\": chunk.page_content,\n",
    "                \"difficulty_hint\": DIFFICULTY_HINT[difficulty]\n",
    "            })\n",
    "            data = json.loads(res)\n",
    "            if isinstance(data, dict):\n",
    "                data = [data]\n",
    "            data = [it for it in data if validate_item(it)]\n",
    "            data =  semantic_filter(data, chunk.page_content)\n",
    "            \n",
    "            for it in data:\n",
    "                it[\"difficulty\"] = difficulty\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"MCQ generation failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=cfg[\"max_workers\"]) as executor:\n",
    "        # Process chunks in batches to allow early stopping\n",
    "        for i in range(0, len(chunks), cfg[\"max_workers\"]):\n",
    "            batch = chunks[i:i + cfg[\"max_workers\"]]\n",
    "            tasks = [loop.run_in_executor(executor, process_chunk, c) for c in batch]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            for sublist in results:\n",
    "                for q in sublist:\n",
    "                    if len(collected) < max_questions:\n",
    "                        collected.append(q)\n",
    "                    else:\n",
    "                        logging.info(f\"Reached {max_questions} MCQs. Stopping early.\")\n",
    "                        return dedupe_items(collected)\n",
    "\n",
    "    return dedupe_items(collected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ac330",
   "metadata": {},
   "source": [
    "### Batch Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c82ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring prompt: LLM rates multiple MCQs in a single call and returns JSON\n",
    "scoring_prompt = PromptTemplate(\n",
    "    input_variables=[\"questions\"],\n",
    "    template=\"\"\"\n",
    "You are grading multiple-choice questions for clarity, quality, and challenge.\n",
    "\n",
    "For each MCQ, assign an integer score 1–5:\n",
    "1 = poor (unclear, trivial, incorrect)\n",
    "3 = acceptable (clear, somewhat useful, minor issues)\n",
    "5 = excellent (clear, challenging, plausible distractors, grounded)\n",
    "\n",
    "Return STRICT JSON (no commentary):\n",
    "[\n",
    "  {{\"question\": \"...\", \"score\": 1}},\n",
    "  ...\n",
    "]\n",
    "\n",
    "MCQs to score (JSON list):\n",
    "{questions}\n",
    "\"\"\"\n",
    ")\n",
    "scoring_chain = LLMChain(llm=llm, prompt=scoring_prompt)\n",
    "\n",
    "\n",
    "def batch_score(items, batch_size=5):\n",
    "    \"\"\"\n",
    "    Score MCQs in batches to reduce number of LLM calls.\n",
    "\n",
    "    Args:\n",
    "        items (list): MCQ dicts\n",
    "        batch_size (int): number of MCQs per scoring call\n",
    "\n",
    "    Returns:\n",
    "        list: MCQs with 'score' field added\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Score MCQs in batches to reduce API calls.\n",
    "    Merges scores back to items by matching 'question' text.\n",
    "    \"\"\"\n",
    "    if not items:\n",
    "        return items\n",
    "\n",
    "    scored = []\n",
    "    for i in tqdm(range(0, len(items), batch_size), desc=\"Scoring batches\"):\n",
    "        batch = items[i:i+batch_size]\n",
    "        try:\n",
    "            res = scoring_chain.run({\"questions\": json.dumps(batch)})\n",
    "            scores = json.loads(res)\n",
    "            # Merge scores back by question text\n",
    "            for it in batch:\n",
    "                match = next((s for s in scores if s.get(\"question\") == it[\"question\"]), None)\n",
    "                it[\"score\"] = int(match.get(\"score\", 1)) if match else 1\n",
    "            scored.extend(batch)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Batch scoring failed, defaulting scores to 1: {e}\")\n",
    "            for it in batch:\n",
    "                it[\"score\"] = 1\n",
    "            scored.extend(batch)\n",
    "    return scored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305ae1f",
   "metadata": {},
   "source": [
    "### Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92562e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_mcqs(items, out_path):\n",
    "    \"\"\"\n",
    "    Save MCQs to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        items(list): List of MCQ dicts.\n",
    "        out_path(str): File path to saved output in JSON.\n",
    "    \"\"\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items, f, indent=2, ensure_ascii=False)\n",
    "    logging.info(f\"Saved {len(items)} MCQs to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7d653",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5181a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-07 17:25:32,556 | WARNING | Ignoring wrong pointing object 193 0 (offset 0)\n",
      "2025-09-07 17:25:32,556 | WARNING | Ignoring wrong pointing object 274 0 (offset 0)\n",
      "2025-09-07 17:25:32,556 | WARNING | Ignoring wrong pointing object 293 0 (offset 0)\n",
      "2025-09-07 17:25:32,556 | WARNING | Ignoring wrong pointing object 331 0 (offset 0)\n",
      "2025-09-07 17:25:32,565 | WARNING | Ignoring wrong pointing object 353 0 (offset 0)\n",
      "2025-09-07 17:25:32,565 | WARNING | Ignoring wrong pointing object 373 0 (offset 0)\n",
      "2025-09-07 17:25:32,565 | WARNING | Ignoring wrong pointing object 394 0 (offset 0)\n",
      "2025-09-07 17:25:32,565 | WARNING | Ignoring wrong pointing object 396 0 (offset 0)\n",
      "2025-09-07 17:25:32,565 | WARNING | Ignoring wrong pointing object 404 0 (offset 0)\n",
      "2025-09-07 17:25:32,565 | WARNING | Ignoring wrong pointing object 418 0 (offset 0)\n",
      "2025-09-07 17:25:32,565 | WARNING | Ignoring wrong pointing object 423 0 (offset 0)\n",
      "2025-09-07 17:25:32,565 | WARNING | Ignoring wrong pointing object 436 0 (offset 0)\n",
      "2025-09-07 17:25:32,573 | WARNING | Ignoring wrong pointing object 443 0 (offset 0)\n",
      "2025-09-07 17:25:32,573 | WARNING | Ignoring wrong pointing object 531 0 (offset 0)\n",
      "2025-09-07 17:25:32,573 | WARNING | Ignoring wrong pointing object 541 0 (offset 0)\n",
      "2025-09-07 17:25:32,573 | WARNING | Ignoring wrong pointing object 545 0 (offset 0)\n",
      "2025-09-07 17:25:32,573 | WARNING | Ignoring wrong pointing object 561 0 (offset 0)\n",
      "2025-09-07 17:25:32,573 | WARNING | Ignoring wrong pointing object 599 0 (offset 0)\n",
      "2025-09-07 17:25:32,573 | WARNING | Ignoring wrong pointing object 601 0 (offset 0)\n",
      "2025-09-07 17:25:32,573 | WARNING | Ignoring wrong pointing object 603 0 (offset 0)\n",
      "2025-09-07 17:25:32,581 | WARNING | Ignoring wrong pointing object 616 0 (offset 0)\n",
      "2025-09-07 17:25:32,581 | WARNING | Ignoring wrong pointing object 618 0 (offset 0)\n",
      "2025-09-07 17:25:32,581 | WARNING | Ignoring wrong pointing object 628 0 (offset 0)\n",
      "2025-09-07 17:25:32,581 | WARNING | Ignoring wrong pointing object 646 0 (offset 0)\n",
      "2025-09-07 17:25:32,581 | WARNING | Ignoring wrong pointing object 666 0 (offset 0)\n",
      "2025-09-07 17:25:32,581 | WARNING | Ignoring wrong pointing object 681 0 (offset 0)\n",
      "2025-09-07 17:25:32,581 | WARNING | Ignoring wrong pointing object 683 0 (offset 0)\n",
      "2025-09-07 17:25:32,581 | WARNING | Ignoring wrong pointing object 685 0 (offset 0)\n",
      "2025-09-07 17:25:32,581 | WARNING | Ignoring wrong pointing object 690 0 (offset 0)\n",
      "2025-09-07 17:25:32,589 | WARNING | Ignoring wrong pointing object 692 0 (offset 0)\n",
      "2025-09-07 17:25:32,589 | WARNING | Ignoring wrong pointing object 694 0 (offset 0)\n",
      "2025-09-07 17:25:32,589 | WARNING | Ignoring wrong pointing object 696 0 (offset 0)\n",
      "2025-09-07 17:25:32,589 | WARNING | Ignoring wrong pointing object 703 0 (offset 0)\n",
      "2025-09-07 17:25:32,589 | WARNING | Ignoring wrong pointing object 705 0 (offset 0)\n",
      "2025-09-07 17:25:32,589 | WARNING | Ignoring wrong pointing object 710 0 (offset 0)\n",
      "2025-09-07 17:25:32,589 | WARNING | Ignoring wrong pointing object 713 0 (offset 0)\n",
      "2025-09-07 17:25:32,589 | WARNING | Ignoring wrong pointing object 716 0 (offset 0)\n",
      "2025-09-07 17:25:32,589 | WARNING | Ignoring wrong pointing object 725 0 (offset 0)\n",
      "2025-09-07 17:25:32,597 | WARNING | Ignoring wrong pointing object 728 0 (offset 0)\n",
      "2025-09-07 17:25:32,597 | WARNING | Ignoring wrong pointing object 730 0 (offset 0)\n",
      "2025-09-07 17:25:32,597 | WARNING | Ignoring wrong pointing object 732 0 (offset 0)\n",
      "2025-09-07 17:25:32,597 | WARNING | Ignoring wrong pointing object 737 0 (offset 0)\n",
      "2025-09-07 17:25:32,597 | WARNING | Ignoring wrong pointing object 742 0 (offset 0)\n",
      "2025-09-07 17:25:32,597 | WARNING | Ignoring wrong pointing object 751 0 (offset 0)\n",
      "2025-09-07 17:25:32,597 | WARNING | Ignoring wrong pointing object 753 0 (offset 0)\n",
      "2025-09-07 17:25:32,597 | WARNING | Ignoring wrong pointing object 755 0 (offset 0)\n",
      "2025-09-07 17:25:32,597 | WARNING | Ignoring wrong pointing object 757 0 (offset 0)\n",
      "2025-09-07 17:25:32,606 | WARNING | Ignoring wrong pointing object 759 0 (offset 0)\n",
      "2025-09-07 17:25:32,606 | WARNING | Ignoring wrong pointing object 761 0 (offset 0)\n",
      "2025-09-07 17:25:32,606 | WARNING | Ignoring wrong pointing object 766 0 (offset 0)\n",
      "2025-09-07 17:25:32,606 | WARNING | Ignoring wrong pointing object 768 0 (offset 0)\n",
      "2025-09-07 17:25:32,606 | WARNING | Ignoring wrong pointing object 770 0 (offset 0)\n",
      "2025-09-07 17:25:32,606 | WARNING | Ignoring wrong pointing object 772 0 (offset 0)\n",
      "2025-09-07 17:25:32,606 | WARNING | Ignoring wrong pointing object 774 0 (offset 0)\n",
      "2025-09-07 17:25:32,614 | WARNING | Ignoring wrong pointing object 776 0 (offset 0)\n",
      "2025-09-07 17:25:32,614 | WARNING | Ignoring wrong pointing object 778 0 (offset 0)\n",
      "2025-09-07 17:25:32,614 | WARNING | Ignoring wrong pointing object 780 0 (offset 0)\n",
      "2025-09-07 17:25:32,614 | WARNING | Ignoring wrong pointing object 788 0 (offset 0)\n",
      "2025-09-07 17:25:32,614 | WARNING | Ignoring wrong pointing object 790 0 (offset 0)\n",
      "2025-09-07 17:25:32,614 | WARNING | Ignoring wrong pointing object 792 0 (offset 0)\n",
      "2025-09-07 17:25:32,614 | WARNING | Ignoring wrong pointing object 794 0 (offset 0)\n",
      "2025-09-07 17:25:32,614 | WARNING | Ignoring wrong pointing object 796 0 (offset 0)\n",
      "2025-09-07 17:25:32,614 | WARNING | Ignoring wrong pointing object 798 0 (offset 0)\n",
      "2025-09-07 17:25:32,622 | WARNING | Ignoring wrong pointing object 800 0 (offset 0)\n",
      "2025-09-07 17:25:32,624 | WARNING | Ignoring wrong pointing object 802 0 (offset 0)\n",
      "2025-09-07 17:25:32,626 | WARNING | Ignoring wrong pointing object 804 0 (offset 0)\n",
      "2025-09-07 17:25:32,626 | WARNING | Ignoring wrong pointing object 812 0 (offset 0)\n",
      "2025-09-07 17:25:32,628 | WARNING | Ignoring wrong pointing object 814 0 (offset 0)\n",
      "2025-09-07 17:25:32,628 | WARNING | Ignoring wrong pointing object 816 0 (offset 0)\n",
      "2025-09-07 17:25:32,628 | WARNING | Ignoring wrong pointing object 826 0 (offset 0)\n",
      "2025-09-07 17:25:32,630 | WARNING | Ignoring wrong pointing object 828 0 (offset 0)\n",
      "2025-09-07 17:25:32,630 | WARNING | Ignoring wrong pointing object 830 0 (offset 0)\n",
      "2025-09-07 17:25:32,630 | WARNING | Ignoring wrong pointing object 837 0 (offset 0)\n",
      "2025-09-07 17:25:32,630 | WARNING | Ignoring wrong pointing object 839 0 (offset 0)\n",
      "2025-09-07 17:25:32,630 | WARNING | Ignoring wrong pointing object 844 0 (offset 0)\n",
      "2025-09-07 17:25:32,630 | WARNING | Ignoring wrong pointing object 846 0 (offset 0)\n",
      "2025-09-07 17:25:32,630 | WARNING | Ignoring wrong pointing object 848 0 (offset 0)\n",
      "2025-09-07 17:25:32,630 | WARNING | Ignoring wrong pointing object 850 0 (offset 0)\n",
      "2025-09-07 17:25:32,638 | WARNING | Ignoring wrong pointing object 914 0 (offset 0)\n",
      "2025-09-07 17:25:32,638 | WARNING | Ignoring wrong pointing object 919 0 (offset 0)\n",
      "2025-09-07 17:25:32,638 | WARNING | Ignoring wrong pointing object 945 0 (offset 0)\n",
      "2025-09-07 17:25:32,638 | WARNING | Ignoring wrong pointing object 959 0 (offset 0)\n",
      "2025-09-07 17:25:32,638 | WARNING | Ignoring wrong pointing object 977 0 (offset 0)\n",
      "2025-09-07 17:25:32,646 | WARNING | Ignoring wrong pointing object 1006 0 (offset 0)\n",
      "2025-09-07 17:25:32,646 | WARNING | Ignoring wrong pointing object 1027 0 (offset 0)\n",
      "2025-09-07 17:25:32,650 | WARNING | Ignoring wrong pointing object 1035 0 (offset 0)\n",
      "2025-09-07 17:25:32,652 | WARNING | Ignoring wrong pointing object 1046 0 (offset 0)\n",
      "2025-09-07 17:25:32,660 | WARNING | Ignoring wrong pointing object 1049 0 (offset 0)\n",
      "2025-09-07 17:25:32,666 | WARNING | Ignoring wrong pointing object 1058 0 (offset 0)\n",
      "2025-09-07 17:25:32,674 | WARNING | Ignoring wrong pointing object 1084 0 (offset 0)\n",
      "2025-09-07 17:25:32,680 | WARNING | Ignoring wrong pointing object 1086 0 (offset 0)\n",
      "2025-09-07 17:25:32,682 | WARNING | Ignoring wrong pointing object 1088 0 (offset 0)\n",
      "2025-09-07 17:25:32,684 | WARNING | Ignoring wrong pointing object 1090 0 (offset 0)\n",
      "2025-09-07 17:25:32,686 | WARNING | Ignoring wrong pointing object 1092 0 (offset 0)\n",
      "2025-09-07 17:25:32,690 | WARNING | Ignoring wrong pointing object 1094 0 (offset 0)\n",
      "2025-09-07 17:25:32,692 | WARNING | Ignoring wrong pointing object 1096 0 (offset 0)\n",
      "2025-09-07 17:25:32,692 | WARNING | Ignoring wrong pointing object 1102 0 (offset 0)\n",
      "2025-09-07 17:25:32,694 | WARNING | Ignoring wrong pointing object 1104 0 (offset 0)\n",
      "2025-09-07 17:25:32,694 | WARNING | Ignoring wrong pointing object 1106 0 (offset 0)\n",
      "2025-09-07 17:25:32,700 | WARNING | Ignoring wrong pointing object 1108 0 (offset 0)\n",
      "2025-09-07 17:25:32,700 | WARNING | Ignoring wrong pointing object 1110 0 (offset 0)\n",
      "2025-09-07 17:25:32,702 | WARNING | Ignoring wrong pointing object 1112 0 (offset 0)\n",
      "2025-09-07 17:25:32,704 | WARNING | Ignoring wrong pointing object 1119 0 (offset 0)\n",
      "2025-09-07 17:25:32,706 | WARNING | Ignoring wrong pointing object 1121 0 (offset 0)\n",
      "2025-09-07 17:25:32,708 | WARNING | Ignoring wrong pointing object 1161 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1163 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1165 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1167 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1169 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1182 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1184 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1186 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1188 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1190 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1192 0 (offset 0)\n",
      "2025-09-07 17:25:32,711 | WARNING | Ignoring wrong pointing object 1194 0 (offset 0)\n",
      "2025-09-07 17:25:32,719 | WARNING | Ignoring wrong pointing object 1200 0 (offset 0)\n",
      "2025-09-07 17:25:32,719 | WARNING | Ignoring wrong pointing object 1202 0 (offset 0)\n",
      "2025-09-07 17:25:32,719 | WARNING | Ignoring wrong pointing object 1204 0 (offset 0)\n",
      "2025-09-07 17:25:32,722 | WARNING | Ignoring wrong pointing object 1206 0 (offset 0)\n",
      "2025-09-07 17:25:32,722 | WARNING | Ignoring wrong pointing object 1208 0 (offset 0)\n",
      "2025-09-07 17:25:32,722 | WARNING | Ignoring wrong pointing object 1210 0 (offset 0)\n",
      "2025-09-07 17:25:32,727 | WARNING | Ignoring wrong pointing object 1212 0 (offset 0)\n",
      "2025-09-07 17:25:32,727 | WARNING | Ignoring wrong pointing object 1215 0 (offset 0)\n",
      "2025-09-07 17:25:32,729 | WARNING | Ignoring wrong pointing object 1217 0 (offset 0)\n",
      "2025-09-07 17:25:32,729 | WARNING | Ignoring wrong pointing object 1219 0 (offset 0)\n",
      "2025-09-07 17:25:32,731 | WARNING | Ignoring wrong pointing object 1221 0 (offset 0)\n",
      "2025-09-07 17:25:32,731 | WARNING | Ignoring wrong pointing object 1223 0 (offset 0)\n",
      "2025-09-07 17:25:32,733 | WARNING | Ignoring wrong pointing object 1225 0 (offset 0)\n",
      "2025-09-07 17:25:32,733 | WARNING | Ignoring wrong pointing object 1227 0 (offset 0)\n",
      "2025-09-07 17:25:32,735 | WARNING | Ignoring wrong pointing object 1229 0 (offset 0)\n",
      "2025-09-07 17:25:32,738 | WARNING | Ignoring wrong pointing object 1231 0 (offset 0)\n",
      "2025-09-07 17:25:32,738 | WARNING | Ignoring wrong pointing object 1233 0 (offset 0)\n",
      "2025-09-07 17:25:32,738 | WARNING | Ignoring wrong pointing object 1235 0 (offset 0)\n",
      "2025-09-07 17:25:32,742 | WARNING | Ignoring wrong pointing object 1237 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1239 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1241 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1252 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1254 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1256 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1258 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1266 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1268 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1270 0 (offset 0)\n",
      "2025-09-07 17:25:32,743 | WARNING | Ignoring wrong pointing object 1297 0 (offset 0)\n",
      "2025-09-07 17:25:32,753 | WARNING | Ignoring wrong pointing object 1310 0 (offset 0)\n",
      "2025-09-07 17:25:32,755 | WARNING | Ignoring wrong pointing object 1314 0 (offset 0)\n",
      "2025-09-07 17:25:32,757 | WARNING | Ignoring wrong pointing object 1318 0 (offset 0)\n",
      "2025-09-07 17:25:32,757 | WARNING | Ignoring wrong pointing object 1323 0 (offset 0)\n",
      "2025-09-07 17:25:32,759 | WARNING | Ignoring wrong pointing object 1344 0 (offset 0)\n",
      "2025-09-07 17:25:32,759 | WARNING | Ignoring wrong pointing object 1346 0 (offset 0)\n",
      "2025-09-07 17:25:32,761 | WARNING | Ignoring wrong pointing object 1348 0 (offset 0)\n",
      "2025-09-07 17:25:32,761 | WARNING | Ignoring wrong pointing object 1358 0 (offset 0)\n",
      "2025-09-07 17:25:32,761 | WARNING | Ignoring wrong pointing object 1371 0 (offset 0)\n",
      "2025-09-07 17:25:32,763 | WARNING | Ignoring wrong pointing object 1373 0 (offset 0)\n",
      "2025-09-07 17:25:32,763 | WARNING | Ignoring wrong pointing object 1383 0 (offset 0)\n",
      "2025-09-07 17:25:32,763 | WARNING | Ignoring wrong pointing object 1394 0 (offset 0)\n",
      "2025-09-07 17:25:32,763 | WARNING | Ignoring wrong pointing object 1398 0 (offset 0)\n",
      "2025-09-07 17:25:32,763 | WARNING | Ignoring wrong pointing object 1413 0 (offset 0)\n",
      "2025-09-07 17:25:32,767 | WARNING | Ignoring wrong pointing object 1417 0 (offset 0)\n",
      "2025-09-07 17:25:32,767 | WARNING | Ignoring wrong pointing object 1427 0 (offset 0)\n",
      "2025-09-07 17:25:32,767 | WARNING | Ignoring wrong pointing object 1429 0 (offset 0)\n",
      "2025-09-07 17:25:32,767 | WARNING | Ignoring wrong pointing object 1468 0 (offset 0)\n",
      "2025-09-07 17:25:32,767 | WARNING | Ignoring wrong pointing object 1470 0 (offset 0)\n",
      "2025-09-07 17:25:32,767 | WARNING | Ignoring wrong pointing object 1472 0 (offset 0)\n",
      "2025-09-07 17:25:32,774 | WARNING | Ignoring wrong pointing object 1474 0 (offset 0)\n",
      "2025-09-07 17:25:32,775 | WARNING | Ignoring wrong pointing object 1499 0 (offset 0)\n",
      "2025-09-07 17:25:32,775 | WARNING | Ignoring wrong pointing object 1504 0 (offset 0)\n",
      "2025-09-07 17:25:32,775 | WARNING | Ignoring wrong pointing object 1708 0 (offset 0)\n",
      "2025-09-07 17:25:32,775 | WARNING | Ignoring wrong pointing object 1711 0 (offset 0)\n",
      "2025-09-07 17:25:32,775 | WARNING | Ignoring wrong pointing object 1769 0 (offset 0)\n",
      "2025-09-07 17:25:32,775 | WARNING | Ignoring wrong pointing object 1771 0 (offset 0)\n",
      "2025-09-07 17:25:32,775 | WARNING | Ignoring wrong pointing object 1773 0 (offset 0)\n",
      "2025-09-07 17:25:32,775 | WARNING | Ignoring wrong pointing object 1775 0 (offset 0)\n",
      "2025-09-07 17:25:32,784 | WARNING | Ignoring wrong pointing object 1780 0 (offset 0)\n",
      "2025-09-07 17:25:32,784 | WARNING | Ignoring wrong pointing object 1782 0 (offset 0)\n",
      "2025-09-07 17:25:32,784 | WARNING | Ignoring wrong pointing object 1784 0 (offset 0)\n",
      "2025-09-07 17:25:32,784 | WARNING | Ignoring wrong pointing object 1786 0 (offset 0)\n",
      "2025-09-07 17:25:32,784 | WARNING | Ignoring wrong pointing object 1796 0 (offset 0)\n",
      "2025-09-07 17:25:32,784 | WARNING | Ignoring wrong pointing object 1798 0 (offset 0)\n",
      "2025-09-07 17:25:32,784 | WARNING | Ignoring wrong pointing object 1800 0 (offset 0)\n",
      "2025-09-07 17:25:32,792 | WARNING | Ignoring wrong pointing object 1802 0 (offset 0)\n",
      "2025-09-07 17:25:32,792 | WARNING | Ignoring wrong pointing object 1804 0 (offset 0)\n",
      "2025-09-07 17:25:32,792 | WARNING | Ignoring wrong pointing object 1821 0 (offset 0)\n",
      "2025-09-07 17:25:32,792 | WARNING | Ignoring wrong pointing object 1830 0 (offset 0)\n",
      "2025-09-07 17:25:32,792 | WARNING | Ignoring wrong pointing object 1835 0 (offset 0)\n",
      "2025-09-07 17:25:32,792 | WARNING | Ignoring wrong pointing object 1844 0 (offset 0)\n",
      "2025-09-07 17:25:32,792 | WARNING | Ignoring wrong pointing object 1849 0 (offset 0)\n",
      "2025-09-07 17:25:32,792 | WARNING | Ignoring wrong pointing object 1907 0 (offset 0)\n",
      "2025-09-07 17:25:44,139 | INFO | Loaded 728 chunks from notes.pdf (skipping pages 5 pages)\n",
      "2025-09-07 17:25:44,139 | INFO | Generating 2 easy questions...\n",
      "C:\\Users\\game4\\AppData\\Local\\Temp\\ipykernel_30636\\4237755889.py:15: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = mcq_chain.run({\n",
      "2025-09-07 17:25:48,408 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-07 17:25:48,413 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-07 17:25:48,415 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.37it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.49it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.76it/s]\n",
      "\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.63it/s]\n",
      "2025-09-07 17:25:49,123 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-07 17:25:49,123 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.48it/s]\n",
      "2025-09-07 17:25:49,446 | INFO | Reached 2 MCQs. Stopping early.\n",
      "2025-09-07 17:25:49,446 | INFO | Generating 4 medium questions...\n",
      "2025-09-07 17:25:49,840 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:25:49,846 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:25:49,846 | INFO | Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "2025-09-07 17:25:49,846 | INFO | Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "2025-09-07 17:25:51,263 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:25:51,263 | INFO | Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "2025-09-07 17:25:52,073 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.95it/s]\n",
      "2025-09-07 17:25:52,884 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-07 17:25:52,889 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.31it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]2025-09-07 17:25:53,134 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:25:53,137 | INFO | Retrying request to /openai/v1/chat/completions in 28.000000 seconds\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.37it/s]\n",
      "2025-09-07 17:25:54,756 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:25:54,756 | WARNING | MCQ generation failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01k4j6pdw2ft48503q5c9r68cy` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Used 11100, Requested 555. Please try again in 27.4165s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "2025-09-07 17:26:23,489 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.05it/s]\n",
      "2025-09-07 17:26:23,702 | INFO | Reached 4 MCQs. Stopping early.\n",
      "2025-09-07 17:26:23,702 | INFO | Generating 4 hard questions...\n",
      "2025-09-07 17:26:23,924 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:23,924 | INFO | Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "2025-09-07 17:26:25,044 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:25,072 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:25,072 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:25,072 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:25,072 | INFO | Retrying request to /openai/v1/chat/completions in 5.000000 seconds\n",
      "2025-09-07 17:26:25,078 | INFO | Retrying request to /openai/v1/chat/completions in 4.000000 seconds\n",
      "2025-09-07 17:26:25,103 | INFO | Retrying request to /openai/v1/chat/completions in 4.000000 seconds\n",
      "2025-09-07 17:26:25,105 | INFO | Retrying request to /openai/v1/chat/completions in 6.000000 seconds\n",
      "2025-09-07 17:26:29,516 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:29,519 | INFO | Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2025-09-07 17:26:30,456 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:30,456 | INFO | Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2025-09-07 17:26:31,392 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:31,400 | INFO | Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "2025-09-07 17:26:31,808 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:31,808 | INFO | Retrying request to /openai/v1/chat/completions in 4.000000 seconds\n",
      "2025-09-07 17:26:32,107 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.68it/s]\n",
      "2025-09-07 17:26:32,674 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:32,682 | WARNING | MCQ generation failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01k4j6pdw2ft48503q5c9r68cy` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Used 9042, Requested 459. Please try again in 11.263999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "2025-09-07 17:26:33,881 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:33,881 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-07 17:26:33,888 | WARNING | MCQ generation failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01k4j6pdw2ft48503q5c9r68cy` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Used 8915, Requested 555. Please try again in 11.029999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.61it/s]\n",
      "2025-09-07 17:26:36,489 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:36,489 | WARNING | MCQ generation failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01k4j6pdw2ft48503q5c9r68cy` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Used 9508, Requested 537. Please try again in 15.3425s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "2025-09-07 17:26:36,492 | INFO | Reached 4 MCQs. Stopping early.\n",
      "Scoring batches:   0%|          | 0/2 [00:00<?, ?it/s]2025-09-07 17:26:37,171 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:26:37,171 | INFO | Retrying request to /openai/v1/chat/completions in 21.000000 seconds\n",
      "2025-09-07 17:27:01,643 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scoring batches:  50%|█████     | 1/2 [00:25<00:25, 25.15s/it]2025-09-07 17:27:01,873 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-07 17:27:01,874 | INFO | Retrying request to /openai/v1/chat/completions in 13.000000 seconds\n",
      "2025-09-07 17:27:21,072 | INFO | HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Scoring batches: 100%|██████████| 2/2 [00:44<00:00, 22.29s/it]\n",
      "2025-09-07 17:27:21,078 | INFO | Saved 10 MCQs to generated_mcqs.json\n",
      "2025-09-07 17:27:21,080 | INFO | Generated 10 balanced MCQs across difficulties.\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    \"\"\"\n",
    "    Orchestrator: load chunks, generate MCQs with early stopping, score them in batches,\n",
    "    sort by score, and save the output. Generate balanced MCQs across difficulties\n",
    "    (based on Bloom’s Taxonomy).\n",
    "    \"\"\"\n",
    "    chunks = load_and_chunk_pdf(\n",
    "        cfg[\"pdf_path\"], \n",
    "        cfg[\"chunk_size\"], \n",
    "        cfg[\"chunk_overlap\"],\n",
    "        cfg[\"skip_pages\"]\n",
    "    )\n",
    "\n",
    "    all_mcqs = []\n",
    "    # Loop over requested difficulty distribution\n",
    "    for diff, num_q in cfg[\"difficulty_distribution\"].items():\n",
    "        logging.info(f\"Generating {num_q} {diff} questions...\")\n",
    "        subset = await generate_mcqs_async(chunks, diff, num_q)\n",
    "        all_mcqs.extend(subset)\n",
    "\n",
    "    # Score and sort combined questions\n",
    "    all_mcqs = batch_score(all_mcqs, cfg[\"batch_size\"])\n",
    "    all_mcqs = sorted(all_mcqs, key=lambda x: x.get(\"score\", 0), reverse=True)\n",
    "    \n",
    "    save_mcqs(all_mcqs, \"generated_mcqs.json\")\n",
    "    logging.info(f\"Generated {len(all_mcqs)} balanced MCQs across difficulties.\")\n",
    "\n",
    "    try:\n",
    "        asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "     pass\n",
    "\n",
    "await main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed5b0d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1: According to the text, what is the rectangular (Cartesian) form of the phasor at 45°? (Score: 4, Difficulty: easy)\n",
      "   1. A. √2/2 + j √2/2 ✅\n",
      "   2. B. 1 + j0  \n",
      "   3. C. 1/2 + j √3/2  \n",
      "   4. D. -√2/2 + j √2/2  \n",
      "Explanation: The table lists the 45° phasor as ε₄ (1↑2 + j 1↑2), which corresponds to cos 45° = √2/2 and sin 45° = √2/2, giving √2/2 + j √2/2.\n",
      "\n",
      "Q2: Given the values in Table 1.1, what is the exact value of \\(\\tan\\frac{\\pi}{6}\\)? (Score: 4, Difficulty: medium)\n",
      "   1. A. \\(\\sqrt{3}\\)  \n",
      "   2. B. \\(\\frac{1}{\\sqrt{3}}\\)  \n",
      "   3. C. \\(\\frac{\\sqrt{3}}{3}\\) ✅\n",
      "   4. D. \\(\\frac{1}{2}\\)  \n",
      "Explanation: From the table, \\(\\sin\\frac{\\pi}{6}=\\frac{1}{2}\\) and \\(\\cos\\frac{\\pi}{6}=\\frac{\\sqrt{3}}{2}\\). Using \\(\\tan\\theta=\\frac{\\sin\\theta}{\\cos\\theta}\\), \\(\\tan\\frac{\\pi}{6}=\\frac{\\frac{1}{2}}{\\frac{\\sqrt{3}}{2}}=\\frac{1}{\\sqrt{3}}=\\frac{\\sqrt{3}}{3}\\).\n",
      "\n",
      "Q3: Given a = 3 and b = 4, what are the magnitude r and angle ϖ according to the formulas r = √(a² + b²) and ϖ = tan⁻¹(b/a)? (Score: 4, Difficulty: medium)\n",
      "   1. A. r = 5, ϖ = tan⁻¹(4/3) ✅\n",
      "   2. B. r = 5, ϖ = tan⁻¹(3/4)  \n",
      "   3. C. r = √13, ϖ = tan⁻¹(4/3)  \n",
      "   4. D. r = 7, ϖ = tan⁻¹(4/3)  \n",
      "Explanation: r = √(3²+4²) = √25 = 5. The angle ϖ = tan⁻¹(b/a) = tan⁻¹(4/3). Only choice A matches both values.\n",
      "\n",
      "Q4: What are the constraints on r and ϖ stated in the text? (Score: 4, Difficulty: hard)\n",
      "   1. A. r ≥ 0 and 0 ≤ ϖ < 2π ✅\n",
      "   2. B. r > 0 and -π ≤ ϖ ≤ π  \n",
      "   3. C. r ≤ 0 and 0 ≤ ϖ < π  \n",
      "   4. D. r > 0 and 0 < ϖ ≤ π/2  \n",
      "Explanation: The passage specifies that the magnitude r is non‑negative (r ≥ 0) and the angle ϖ lies in the half‑open interval from 0 inclusive to 2π exclusive. The other options impose different sign or range restrictions that are not consistent with the given definition.\n",
      "\n",
      "Q5: According to the passage, under what condition must a student use complex numbers when solving the quadratic equation ax² + bx + c = 0? (Score: 4, Difficulty: hard)\n",
      "   1. A. When a = 0  \n",
      "   2. B. When b² − 4ac > 0  \n",
      "   3. C. When b² − 4ac = 0  \n",
      "   4. D. When b² − 4ac < 0 ✅\n",
      "Explanation: The text explains that if the discriminant b² − 4ac is negative, we must take the square root of a negative number, which requires the use of complex numbers.\n"
     ]
    }
   ],
   "source": [
    "def preview_mcqs(items, n=5):\n",
    "    \"\"\"\n",
    "    Print a preview of the top N MCQs with their score and difficulty.\n",
    "\n",
    "    Args:\n",
    "        items (list): List of MCQ dicts\n",
    "        n (int): Number of MCQs to preview\n",
    "    \"\"\"\n",
    "    for i, it in enumerate(items[:n], 1):\n",
    "        print(f\"\\nQ{i}: {it['question']} (Score: {it.get('score', 'N/A')}, Difficulty: {it.get('difficulty', 'N/A')})\")\n",
    "        for idx, choice in enumerate(it['choices'], 1):\n",
    "            marker = \"✅\" if choice == it[\"answer\"] else \" \"\n",
    "            print(f\"   {idx}. {choice} {marker}\")\n",
    "        print(f\"Explanation: {it['explanation']}\")\n",
    "\n",
    "# Load JSON and preview sample questions\n",
    "with open(\"generated_mcqs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mcqs = json.load(f)\n",
    "\n",
    "preview_mcqs(mcqs, n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b43f5",
   "metadata": {},
   "source": [
    "### ⚡ Performance Notes: Async & Batching\n",
    "\n",
    "This pipeline is optimized for speed using:\n",
    "\n",
    "1. **Async MCQ Generation**  \n",
    "   - Multiple chunks are processed in parallel using `ThreadPoolExecutor` + `asyncio`.  \n",
    "   - Configurable via `cfg[\"max_workers\"]`.  \n",
    "   - Example: `max_workers=5` → 5 llm calls at once.  \n",
    "\n",
    "2. **Batch Scoring**  \n",
    "   - Instead of scoring one MCQ at a time, MCQs are grouped into batches.  \n",
    "   - Reduces API calls significantly.  \n",
    "   - Configurable via `cfg[\"batch_size\"]`.  \n",
    "   - Example: `batch_size=5` → 5 MCQs scored per Gemini call.  \n",
    "\n",
    "3. **Sentence Transformer Filtering**  \n",
    "   - Semantic filtering is done **locally** with `all-MiniLM-L6-v2`.  \n",
    "   - This avoids extra API calls and speeds up processing.  \n",
    "\n",
    "4. **Early stopping**\n",
    "   - Halts as soon as `max_questions` are generated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db1802",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
